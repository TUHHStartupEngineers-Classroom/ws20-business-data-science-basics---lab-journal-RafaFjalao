---
title: "Journal (reproducible report)"
author: "Rafael Fernandez Jalao"
date: "2020-11-05"
output:
  html_document:
    toc: true
    toc_float: true
    collapsed: false
    number_sections: true
    toc_depth: 3
    #code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE,warning=FALSE, cache=TRUE)
```

THIS JOURNAL IS THE RESOLUTION OF THE ASSIGNMENTS FROM THE SUBJECT BUSINESS DATA BASICS: THEY ARE SOLD BY RAFAEL FERNANDEZ JALAO  (497057)

Last compiled: `r Sys.Date()`

# FIRST ASSIGNMENT

In this first assignment we had to analyze the sales of bikes in the different states of Germany. There are two challenges: analyze the sales by location (state) with a bar plot and the sales by location and year.

## Data manipulation and cleaning
This part is common for both challenges
```{r}
library(tidyverse)
library(readr)
library(lubridate)

bikes_tbl <- read_excel(path='00_data/01_bike_sales/01_raw_data/bikes.xlsx')
orderlines_tbl <- read_excel(path='00_data/01_bike_sales/01_raw_data/orderlines.xlsx')
bikeshops_tbl <-read_excel(path='00_data/01_bike_sales/01_raw_data/bikeshops.xlsx')


bike_orderlines_joined_tbl <- orderlines_tbl %>%
  left_join(bikes_tbl, by = c("product.id" = "bike.id")) %>%
  left_join(bikeshops_tbl, by = c("customer.id" = "bikeshop.id"))

bike_orderlines_wrangled_tbl<- bike_orderlines_joined_tbl %>%
  #Separate State and city
  separate(col= location, into=c ("city", "state"), sep = ',') %>%
  # Selection of necessary data
  select (...1, order.id, order.date, quantity, price, state)%>%
  #Total price
  mutate (total_price = price*quantity)%>%
  #Correcting the point
  set_names(names(.) %>% str_replace_all("\\.", "_"))
```

## first challenge: Sales by states 
```{r}
#Step 1: Preparing the represented dat<
sales_by_state<- bike_orderlines_wrangled_tbl %>%
  #Selecting the necessary data
  select (state, total_price)%>%
  group_by(state)%>%
  summarize(sales= sum(total_price))%>%
  mutate(sales_text = scales::dollar(sales, big.mark = ".", 
                                     decimal.mark = ",", 
                                     prefix = "", 
                                     suffix = " €"))
```
```{r plot1, fig.width=12, fig.height=6}
sales_by_state%>%
  ggplot(aes(x=state, y=sales ))+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  # Geometries
  geom_col(fill = "#2DC6D6") + # Use geom_col for a bar plot
  geom_label(aes(label = sales_text)) +
  # Adding labels to the bars

  scale_y_continuous(labels = scales::dollar_format(big.mark = ".", 
                                                    decimal.mark = ",", 
                                                    prefix = "", 
                                                    suffix = " €")) +

  labs(
    title    = "SALES BY STATE",
    subtitle = "",
    x = "", # Override defaults for x and y
    y = "Revenue"
  )
```

## Second challenge: Sales by State and Year 
```{r}
sales_state_year <- bike_orderlines_wrangled_tbl %>%
  
  select(order_date, total_price, state) %>%
  mutate (year = year(order_date))%>%
  group_by(year, state)%>%
  summarise(sales=sum(total_price))%>%
  ungroup() %>%
  mutate(sales_text = scales::dollar(sales, big.mark = ".", 
                                     decimal.mark = ",", 
                                     prefix = "", 
                                     suffix = " €"))


```
```{r plot2, fig.width=10, fig.height=10}
sales_state_year%>%
  
  # Set up x, y, fill
  ggplot(aes(x = year, y = sales, fill = state)) +
  
  # Geometries
  geom_col() + # Run up to here to get a stacked bar plot
  
  # Facet
  facet_wrap(~state) +
  
  # Formatting
  scale_y_continuous(labels = scales::dollar_format(big.mark = ".", 
                                                    decimal.mark = ",", 
                                                    prefix = "", 
                                                    suffix = " €")) +
  labs(
    title = "REVENUE BY YEAR AND STATE",
    subtitle = "",
    fill = "States" )# Changes the legend name

```

# SECOND ASSIGMMENT

This assignment is related with the Data Acquisition. In this assignment there are two challenges: Get some data via an API and scrape one of the competitor websites of canyon and create a data base which contain the model names and prices for at least one category

## First Challenge:APIs 
 
In this first challenge we have to collect some data from a API on the internet. I choosed to collect information from Twitter. I followed the step and used the library rtweet. Just for fun, I took the 5000 first tweets, which contain "Election Fraud" and mention the account of Donald Trump.
The code is as follows:
```{r}
library(httr)
library (rtweet)
library(ggplot2)
library(dplyr)
library(tidytext)
library (keyring)
library(RSQLite)

#To secure my credentials ( I am not sure if I made this right)

alphavantage_api_url <- "https://www.alphavantage.co/query"
ticker               <- "WDI.DE"

keyring::key_set("token")
GET(alphavantage_api_url, query = list('function' = "GLOBAL_QUOTE",
                                       symbol     = ticker,
                                       apikey     = key_get("token")))


#Getting the information from the API

appname <- "myfirstdataanalysis"
key <- "S7toD7UeyzhvAh7vDFs1UlIJu"
secret <- "TZlLx5Ao4ZSGTMUrqvdBGulFO33XeUHDRYKbhUE6c6mCUHAV2G"

access_token <- "1325156787662696448-y9q7ebO1fX6050yxttGnZOqcQIWMRH"

access_secret <- "Xw0bLMOfqCtn5c9VxYJ1AkSDstd0Uawq0zHyjxPv1Fslx"

twitter_token <- create_token(app = appname,consumer_key = key,consumer_secret = secret,access_token = access_token,access_secret = access_secret)

#the first 5000 Tweets that used Election Fraud and mention @realDonaldTrump 

election_fraud_tweets <- search_tweets(q = "Election Fraud @realDonaldTrump", include_rts = FALSE, n = 5000)

#Data
head(election_fraud_tweets, n = 100)
```

To get a better idea of the data, I represented the top 10 location of this kind of tweets

```{r plot3, fig.width=12, fig.height=12}
selection <-election_fraud_tweets%>%
  count(location, sort = TRUE) %>%
  mutate(location = reorder(location, n)) %>%
  top_n(11)
# Plots the top 10 Locations   
 selection[-c(1),] %>%
  ggplot(aes(x = location, y = n)) +
  geom_col() +
  coord_flip() +
  labs(x = "Count",
       y = "Location",
       title = "TOP 10 LOCATIONS") 

```


## Second Challenge: SCRAPE A WEBPAGE

In this second challenge we had to  crape one of the competitor websites of canyon and create a data base which contain the model names and prices for at least one category. I have chosen the competitor Rose Bikes. 
I have decided to do it to all categories. 
For each category, I will  get the price for a model(represented in the web-page with from €...). The process could go a little bit deeper and get the price for every variation of the model. However, due to the limitation of time, I will just take the base price for each model. I named every category as family.

```{r}
library(tidyverse) # Main Package - Loads dplyr, purrr, etc.
library(rvest)     # HTML Hacking & Web Scraping
library(xopen)     # Quickly opening URLs
library(jsonlite)  # converts JSON files to R objects
library(glue)      # concatenate strings
library(stringi) 
library(dplyr)

# Second Challenge

url_home <- "https://www.rosebikes.com/bikes"

html_home <- url_home %>%
  read_html()

family_tbl <- html_home%>%
  html_nodes(css=".main-navigation-category-with-tiles__title") %>%
  html_text()%>%
  discard(.p = ~stringr::str_detect(.x,"Urban|Sale|Kids|Bikes"))%>% #deleting
  as_tibble()

ivector <- as_vector(1:nrow(family_tbl)) # create a vector for the interation 

family_name_tbl <- as_tibble()

family_name_price_tbl<- as_tibble()
bikes_data_tbl <- as_tibble()
for (i in ivector){
family_name_tbl<- as_tibble() 
name <- family_tbl[i,1]%>%
  str_replace_all("\n", "")
  

url_home <- glue("https://www.rosebikes.com/bikes/{name}")


html<- url_home %>%
  read_html

model_name_tbl<- html%>%
  html_nodes(css=".catalog-category-bikes__title-text") %>%
  html_text()%>%
  str_replace_all(pattern="\n",replacement = "")%>%
  as_tibble()%>%
  rename(model_name=value)

price_tbl <- html%>%
  html_nodes(css=".catalog-category-bikes__price-title") %>%
  html_text()%>%
  str_replace_all(pattern=",",replacement = "")%>%  # Delete the comma
  str_replace_all(pattern="\n",replacement = "")%>% # delete the \n
  str_replace_all(pattern="from €",replacement = "")%>%
  str_replace_all(pattern=".00",replacement = "")%>%
  readr::parse_number()%>%
  as_tibble()



ivector_family<- as_vector(1:nrow(price_tbl))

for (i in ivector_family){
  
  family_name_tbl[i,1]<-name
}

family_name_tbl <-family_name_tbl%>%
  rename (family_name=...1)

family_name_price_tbl<- bind_cols(family_name_tbl, model_name_tbl, price_tbl)

bikes_data_tbl<- bind_rows(bikes_data_tbl, family_name_price_tbl)


}                                         

print(bikes_data_tbl)

#More values to ilustrate

bikes_data_tbl[23,1]

bikes_data_tbl[23,2]

bikes_data_tbl[23,3]

bikes_data_tbl[30,1]

bikes_data_tbl[30,2]

bikes_data_tbl[30,3]
```

# THIRD ASSIGNMENT

This assignment Cover the Data Wrangling topic. Using a different data from patents, we had to answer the followign questions using data.table or dplyr: 

Patent Dominance: What US company / corporation has the most patents? List the 10 US companies with the most assigned/granted patents.
Recent patent acitivity: What US company had the most patents granted in 2019? List the top 10 companies with the most new granted patents for 2019.
Innovation in Tech: What is the most innovative tech sector? For the top 10 companies (worldwide) with the most patents, what are the top 5 USPTO tech main classes?

## QUESTION 1: PATENT DOMINANCE

The process used is explanied in the following code: 

```{r eval= F}
library(tidyverse)
library(vroom)
library(data.table)
library(vroom)

col_types <- list(
  id = col_character(),
  type = col_character(),
  number = col_character(),
  country = col_character(),
  date = col_date("%Y-%m-%d"),
  abstract = col_character(),
  title = col_character(),
  kind = col_character(),
  num_claims = col_double(),
  filename = col_character(),
  withdrawn = col_double()
)

#Import all the necessary data to answer the question of the challenge

#patent

patent_tbl <- vroom(
  file       = "challenge3/patent.tsv", 
  delim      = "\t", 
  col_types  = col_types,
  na         = c("", "NA", "NULL"))

# Transform the tbl into a dt

patent_dt <- setDT(patent_tbl)

rm(patent_tbl)

#patent assignee

patent_assignee_tbl <- vroom(
  file       = "challenge3/patent_assignee.tsv", 
  delim      = "\t", 
  col_types  = col_types,
  na         = c("", "NA", "NULL"))

# Transform the tbl into a dt

patent_assignee_dt <- setDT(patent_assignee_tbl)


rm(patent_assignee_tbl)

#assignee

assignee_tbl <- vroom(
  file       = "challenge3/assignee.tsv", 
  delim      = "\t", 
  col_types  = col_types,
  na         = c("", "NA", "NULL"))

# Transform the tbl into a dt

assignee_dt <- setDT(assignee_tbl)


rm(assignee_tbl)

#uspc

uspc_tbl <- vroom(
  file       = "challenge3/uspc.tsv", 
  delim      = "\t", 
  col_types  = col_types,
  na         = c("", "NA", "NULL"))

# Transform the tbl into a dt

uspc_dt <- setDT(uspc_tbl)


rm(uspc_tbl)

#QUESTION 1: PATENT DOMINANCE 

# Analysing the excel with the dictonary, we can see that in the data element
#type, we can select the kind of patent( 2 for US company or coorporation)
#So I will start doing that to reduce the unnecessary data 

assignee_US_dt <- assignee_dt[type==2, list(id, organization)][
  , .(assignee_id = id, organization = organization)]

#Combination both tables

assignee_patent_assignee_US_dt <-merge(x = assignee_US_dt, y = patent_assignee_dt, 
                                        by    = "assignee_id", 
                                        all.x = FALSE,  # we just keep the ones
                                        all.y = FALSE)  # which appears on both
                                                        #sides


#First I create a new column with all 1, to count the number of patents

totalnumber_patent_US_dt <- assignee_patent_assignee_US_dt[,cal:=1]

# Now I make the total of patents and ordering descending

totalnumber_patent_US_dt <-totalnumber_patent_US_dt[
  , sum(cal), by = organization][
    order(V1, decreasing = TRUE)][,
       .(organization = organization, number_patents = V1)]

top_10 <- totalnumber_patent_US_dt[1:10,.(organization, number_patents)] 

write_rds(top_10,"top_10.rds")
```
```{r}
top_10
```

## QUESTION 2: RECENT PATENT ACTIVITY

The process used is explanied in the following code: 
```{r eval=FALSE}
#QUESTION 2: RECENT PATENT ACTIVITY


# first, I will select the necessary data from patent 

patent_mod_dt <- patent_dt[, .(patent_id=id, date=date)]

# Now I combine the US assignee patent with the date through the patent id
# Just keeping the commom elements will just give us the companies in the USA

patent_date_US_dt <- merge(x = assignee_patent_assignee_US_dt, y = patent_mod_dt, 
                           by    = "patent_id", 
                           all.x = FALSE,  # we just keep the ones
                           all.y = FALSE)  # which appears on both


#Now I will just keep the id of the patents, the ones granted in 2019,the 
#name of the company and the operator cal (I will use it later)
# I use the library lubricate to select the year 
patent_US_2019_dt <- patent_date_US_dt[lubridate::year(date) == "2019", .(organization, date, patent_id)][
  , cal:= 1]


# Now I make the total of patents and ordering descending

totalnumber_patent_US_2019_dt <-patent_US_2019_dt[
  , sum(cal), by = organization][
    order(V1, decreasing = TRUE)][,
  .(organization = organization, number_patents = V1)]

top_10_US_2019 <- totalnumber_patent_US_2019_dt[1:10,.(organization, number_patents)] 

write_rds(top_10_US_2019, "top_10_US_2019.rds")

```
```{r}
top_10_US_2019
```

## QUESTION 3: INNOVATION IN TECH

The process used is explanied in the following code: 
```{r eval=FALSE}
#QUESTION 3: INNOVATION IN TECH 

#question 3.1: What is the most innovative sector?

# for that I will analyse the uspc_dt
# for that I only need the mainclass variable, and I will add the cal(like before)

uspc_mainclass_dt <- uspc_dt[,.(mainclass_id)][,cal:=1]

# Now I will sum and get the most innovative sector

uspc_innovative_sector <- uspc_mainclass_dt[,sum(cal), by= mainclass_id][
  order(V1, decreasing = TRUE)][,.(mainclass_id=mainclass_id, number_patents = V1)]

top_innovative_sector <- uspc_innovative_sector[1,.(mainclass_id, number_patents)]

write_rds(top_innovative_sector, "top_innovative_sector.rds")

```
```{r}
top_innovative_sector
```
Correspond with ACTIVE SOLID-STATE DEVICES
```{r eval=FALSE}
#Question 3.2: 

#First the top 10 WW 
# Similar process like question 1

assignee_WW_dt <- assignee_dt[,.(assignee_id= id, organization = organization)]

assignee_patent_assignee_WW_dt <-merge(x = assignee_WW_dt, y = patent_assignee_dt, 
                                       by    = "assignee_id", 
                                       all.x = FALSE,  # we just keep the ones
                                       all.y = FALSE) 


#Now let's get the top 10 companies worldwide
#adding first the cal 

assignee_patent_assignee_WW_dt <- assignee_patent_assignee_WW_dt[,cal:=1]

# Now I make the total of patents and ordering descending

totalnumber_patent_WW_dt <-assignee_patent_assignee_WW_dt[
  , sum(cal), by = organization][
    order(V1, decreasing = TRUE)][,
                                  .(organization = organization, number_patents = V1)]

top_10_WW <- totalnumber_patent_WW_dt[1:10,.(organization, number_patents)] 

write_rds(top_10_WW, "top_10_WW.rds")
```
```{r}
top_10_WW
```
```{r eval=FALSE}
# Now the USPC comes into play. We merge all data together

uspc_assignee_patent_assignee_WW_dt <-merge(x = assignee_patent_assignee_WW_dt, y = uspc_dt, 
                                                        by    = "patent_id", 
                                                        all.x = FALSE,  # we just keep the ones
                                                        all.y = FALSE)  # which appears on both



#Cleaning up the data

uspc_assignee_patent_assignee_WW_mod_dt <- uspc_assignee_patent_assignee_WW_dt[,
            .(organization, mainclass_id)][,cal:=1]


# Now we used the top 10 WW obtained before “skipping the one with no name 
#to prevent from errors and introduce it here

uspc_assignee_patent_assignee_top_10_WW_dt<- uspc_assignee_patent_assignee_WW_mod_dt[organization %in%
top_10_WW[!4, organization], sum(cal),by=mainclass_id][
  order(V1, decreasing = TRUE)][,
                                .(mainclass_id = mainclass_id, number_patents = V1)]

#Now I get the top 5 Sector

top_5_sector <- uspc_assignee_patent_assignee_top_10_WW_dt[1:5, .(mainclass_id, number_patents)]

write_rds(top_5_sector, "top_5_sector.rds")
```
```{r}
top_5_sector
```

Which corresponds with:

257: ACTIVE SOLID-STATE DEVICES
438: SEMICONDUCTOR DEVICE MANUFACTURING: PROCESS
365: STATIC INFORMATION STORAGE AND RETRIEVAL
370: MULTIPLEX COMMUNICATIONS
358: FACSIMILE AND STATIC PRESENTATION PROCESSING

# FOURTH ASSIGNMMENT

To close this subject we were challenge to make some data Visualization. There were
two challenges.

## First challenge: CUMULATIVE CASES COVID 2020

First, we have to represent the acumulative cases of corona during 2020 in Spain, Germany, UK, France and USA. 

The solution is explained in the following code:

```{r}
library(tidyverse)
library(lubridate)
library(scales)
library(maps)
covid_data_tbl <- read_csv("https://opendata.ecdc.europa.eu/covid19/casedistribution/csv")


# Graphic 1
#Data Wrangling 
# I had a problem with the dates, I dont know what happen, but I tried everything
# and it kept giving me the false results.
#So for this point, I made it the best I could


covid_data_spain_tbl <- covid_data_tbl%>%
  select("dateRep","day", "month", "year", "cases", "countriesAndTerritories", "continentExp")%>%
  rename("country"="countriesAndTerritories", "continent"= "continentExp", "date"="dateRep")%>%
  filter(year==2020)%>%
  filter(country%>% str_detect("Spain"))%>%
  arrange(day)%>%
  arrange(month)%>%
  mutate(acum = cumsum(cases))%>%
  select("acum","country")

covid_data_spain_tbl<-covid_data_spain_tbl%>%mutate(orden= 1:nrow(covid_data_spain_tbl)) 
  

covid_data_germany_tbl <- covid_data_tbl%>%
  select("dateRep","day", "month", "year", "cases", "countriesAndTerritories", "continentExp")%>%
  rename("country"="countriesAndTerritories", "continent"= "continentExp", "date"="dateRep")%>%
  filter(year==2020)%>%
  filter(country%>% str_detect("Germany"))%>%
    arrange(day)%>%
    arrange(month)%>%
    mutate(acum = cumsum(cases))%>%
    select("acum","country")

covid_data_germany_tbl<- covid_data_germany_tbl%>%
  mutate(orden= 1:nrow(covid_data_germany_tbl))
covid_data_germany_tbl<- covid_data_germany_tbl[-c(339),]

covid_data_france_tbl <- covid_data_tbl%>%
  select("dateRep","day", "month", "year", "cases", "countriesAndTerritories", "continentExp")%>%
  rename("country"="countriesAndTerritories", "continent"= "continentExp", "date"="dateRep")%>%
  filter(year==2020)%>%
  filter(country%>% str_detect("France"))%>%
  arrange(day)%>%
  arrange(month)%>%
  mutate(acum = cumsum(cases))%>%
  select("acum","country")

covid_data_france_tbl<- covid_data_france_tbl%>%
  mutate(orden= 1:nrow(covid_data_france_tbl))
covid_data_france_tbl<- covid_data_france_tbl[-c(339),]

covid_data_UK_tbl <- covid_data_tbl%>%
  select("dateRep","day", "month", "year", "cases", "countriesAndTerritories", "continentExp")%>%
  rename("country"="countriesAndTerritories", "continent"= "continentExp", "date"="dateRep")%>%
  filter(year==2020)%>%
  filter(country%>% str_detect("United_Kingdom"))%>%
  arrange(day)%>%
  arrange(month)%>%
  mutate(acum = cumsum(cases))%>%
  select("acum", "country")
  
covid_data_UK_tbl<-covid_data_UK_tbl%>%
  mutate(orden= 1:nrow(covid_data_UK_tbl))
covid_data_UK_tbl<- covid_data_UK_tbl[-c(339),]



covid_data_USA_tbl <- covid_data_tbl%>%
  select("dateRep","day", "month", "year", "cases", "countriesAndTerritories", "continentExp")%>%
  rename("country"="countriesAndTerritories", "continent"= "continentExp", "date"="dateRep")%>%
  filter(year==2020)%>%
  filter(country%>% str_detect("United_States_of_America"))%>%
  arrange(day)%>%
  arrange(month)%>%
  mutate(acum = cumsum(cases))%>%
  select("acum","country")

covid_data_USA_tbl<-covid_data_USA_tbl%>%
  mutate(orden= 1:nrow(covid_data_USA_tbl))

covid_data_USA_tbl<- covid_data_USA_tbl[-c(339),]


for_graph <- bind_rows( covid_data_UK_tbl, covid_data_USA_tbl, covid_data_france_tbl,covid_data_germany_tbl, covid_data_spain_tbl)

```
```{r plot4, fig.width=12, fig.height=12}
for_graph%>%
  ggplot(aes(x=orden, y= acum, color = country))+
  geom_line(size = 1, linetype = 1)+
  scale_color_manual(values=c("yellow2", "slateblue","salmon2", "purple", "red3"))+
  labs(
    title = "Coronavirus in Spain, France, Germany, UK and USA 2020",
    subtitle = "",
    caption = "04.12.2020",
    x = "Days",
    y = "Cumulative cases",
    color = "")+
  theme(legend.position  = "bottom", 
        legend.direction = "horizontal",
        axis.text.x = element_text(size=5),
        axis.text.y = element_text(size=5))
```


## Second Challenge: Mortality of covid in Map

In the second and last challenge of this subject, we had to represent the mortality per population rate in a word map
```{r}
#Graphic 2

#Data Wrangling
world <- map_data("world")

covid_data_mod <- covid_data_tbl%>%
  select("deaths", "countriesAndTerritories", "popData2019")%>%
  rename("region"="countriesAndTerritories", "deaths"="deaths", "population"="popData2019")%>%
  mutate(across(region, str_replace_all, "_", " "))%>%
  mutate(region = case_when(
    region == "United Kingdom" ~ "UK",
    region == "United States of America" ~ "USA",
    region == "Czechia" ~ "Czech Republic",
    TRUE ~ region ))%>%
  group_by(region)%>%
  summarise(total_deaths= sum(deaths/population))%>%
  ungroup()

plot_data <- left_join(covid_data_mod, world, by = "region")
```
```{r plot5, fig.width=12, fig.height=12}
ggplot(plot_data, aes(map_id = region, fill =total_deaths*100 ))+
  geom_map(map = plot_data,  color = "red")+
  expand_limits(x = plot_data$long, y = plot_data$lat)+
  scale_fill_viridis_c(
    alpha = 1,
    begin = 0,
    end = 1,
    direction = 1,
    option = "C",
    values = NULL,
    space = "Lab",
    na.value = "grey50",
    guide = "colourbar",
    aesthetics = "fill")+labs(
    title = "Confirmed COVID-19 deaths to the size of the population",
    subtitle = "",
    caption = "04.12.2020",
    x = "",
    y = "",
    fill= ("Mortality rate in %"))+
  theme(legend.position  = "top", 
        legend.direction = "horizontal",
        axis.text.x = element_text(size=0),
        axis.text.y = element_text(size=0))
```







